{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ft3yKDEGX--3"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bs0Aa4-3nhIw"},"outputs":[],"source":["%cd /content/drive/MyDrive/Generative\\ Artisan/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mpSqnFVAYJ1s"},"outputs":[],"source":["!pip install git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WisM8XgeiHH3"},"outputs":[],"source":["import os\n","import sys \n","import matplotlib.pyplot as plt \n","\n","from PIL import Image\n","import numpy as np\n","import sys\n","import torch\n","import torch.nn\n","import torch.optim as optim\n","from torchvision import transforms, models\n","\n","import StyleNet\n","import utils\n","import clip\n","import torch.nn.functional as F\n","from template import imagenet_templates\n","\n","from PIL import Image \n","import PIL \n","from torchvision import utils as vutils\n","import argparse\n","from torchvision.transforms.functional import adjust_contrast\n","import random\n","import copy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1GNYLQ5ymx4L"},"outputs":[],"source":["def img_denormalize(image):\n","    mean=torch.tensor([0.485, 0.456, 0.406]).to(device)\n","    std=torch.tensor([0.229, 0.224, 0.225]).to(device)\n","    mean = mean.view(1,-1,1,1)\n","    std = std.view(1,-1,1,1)\n","    image = image * std + mean\n","    return image\n","\n","def img_normalize(image):\n","    mean=torch.tensor([0.485, 0.456, 0.406]).to(device)\n","    std=torch.tensor([0.229, 0.224, 0.225]).to(device)\n","    mean = mean.view(1,-1,1,1)\n","    std = std.view(1,-1,1,1)\n","    image = (image - mean) / std\n","    return image\n","\n","def clip_normalize(image,device):\n","    image = F.interpolate(image,size=224, mode='bicubic', align_corners=False)\n","    mean=torch.tensor([0.48145466, 0.4578275, 0.40821073]).to(device)\n","    std=torch.tensor([0.26862954, 0.26130258, 0.27577711]).to(device)\n","    mean = mean.view(1,-1,1,1)\n","    std = std.view(1,-1,1,1)\n","    image = (image-mean)/std\n","    return image\n","\n","def get_image_prior_losses(inputs_jit):\n","    diff1 = inputs_jit[:, :, :, :-1] - inputs_jit[:, :, :, 1:]\n","    diff2 = inputs_jit[:, :, :-1, :] - inputs_jit[:, :, 1:, :]\n","    diff3 = inputs_jit[:, :, 1:, :-1] - inputs_jit[:, :, :-1, 1:]\n","    diff4 = inputs_jit[:, :, :-1, :-1] - inputs_jit[:, :, 1:, 1:]\n","    loss_var_l2 = torch.norm(diff1) + torch.norm(diff2) + torch.norm(diff3) + torch.norm(diff4)\n","    return loss_var_l2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r6SOB9Mn-nRQ"},"outputs":[],"source":["def decode_segmap(image, nc=21):\n","  label_colors = np.array([(0, 0, 0),  # 0=background\n","               # 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle\n","               (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128),\n","               # 6=bus, 7=car, 8=cat, 9=chair, 10=cow\n","               (0, 128, 128), (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0),\n","               # 11=dining table, 12=dog, 13=horse, 14=motorbike, 15=person\n","               (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128), (192, 128, 128),\n","               # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor\n","               (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64, 128)])\n","  r = np.zeros_like(image).astype(np.uint8)\n","  g = np.zeros_like(image).astype(np.uint8)\n","  b = np.zeros_like(image).astype(np.uint8)\n","  for l in range(0, nc):\n","    idx = image == l\n","    r[idx] = label_colors[l, 0]\n","    g[idx] = label_colors[l, 1]\n","    b[idx] = label_colors[l, 2] \n","  rgb = np.stack([r, g, b], axis=2)\n","  return rgb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UppnXYIrjQZC"},"outputs":[],"source":["def compose_text_with_templates(text, templates=imagenet_templates):\n","    return [template.format(text) for template in templates]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m5NXjx3z8BjK"},"outputs":[],"source":["fcn = models.segmentation.fcn_resnet101(pretrained=True).eval()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","fcn = fcn.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SZw8k3RM8Zag"},"outputs":[],"source":["image = utils.load_image2('./test_set/lena.png', img_size=512)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","img = img_normalize(image.to(device))\n","seg = torch.argmax(fcn(img)['out'].squeeze(), dim=0).detach().cpu().numpy()\n","rgb = decode_segmap(seg)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YxGNxNtQ_stA"},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W-LWAU2C-usL"},"outputs":[],"source":["plt.figure(figsize=(15, 15))\n","plt.imshow(rgb)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"moPdQsCd841M"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","VGG = models.vgg19(pretrained=True).features\n","VGG.to(device)\n","for parameter in VGG.parameters():\n","    parameter.requires_grad_(False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dpFgNXsh8lFk"},"outputs":[],"source":["clip_model, preprocess = clip.load('ViT-B/32', device, jit=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V47EZ1Xki6yG"},"outputs":[],"source":["from IPython.display import display\n","from argparse import Namespace\n","\n","source = \"a Photo\"\n","text = \"Starry Night by Vincent van gogh\"\n","#text = \"The great wave off Wanagawa by Hokusai\"\n","#text = \"The scream by edvard munch\"\n","\n","crop_size = 128\n","image_dir = \"./test_set/lena.png\"\n","\n","training_iterations = 400\n","\n","training_args = {\n","    \"lambda_tv\": 2e-3,\n","    \"lambda_patch\": 9000,\n","    \"lambda_dir\": 500,\n","    \"lambda_c\": 150,\n","    \"crop_size\": 128,\n","    \"num_crops\": 64,\n","    \"img_size\": 512,\n","    \"max_step\": training_iterations,\n","    \"lr\": 5e-4,\n","    \"thresh\": 0.7,\n","    \"content_path\": image_dir,\n","    \"text\": text\n","}\n","\n","args = Namespace(**training_args)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_EEATuKejLfZ"},"outputs":[],"source":["content_path = args.content_path\n","content_image = utils.load_image2(content_path, img_size=args.img_size)\n","content_image = content_image.to(device)\n","content_features = utils.get_features(img_normalize(content_image), VGG)\n","target = content_image.clone().requires_grad_(True).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"poyhOkoDkYNv"},"outputs":[],"source":["style_net = StyleNet.UNet()\n","style_net.to(device)\n","\n","content_weight = args.lambda_c\n","crop_size = args.crop_size\n","\n","optimizer = optim.Adam(style_net.parameters(), lr=args.lr)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n","steps = args.max_step\n","\n","content_loss_epoch = []\n","style_loss_epoch = []\n","total_loss_epoch = []\n","\n","cropper = transforms.RandomCrop(args.crop_size)\n","\n","augment = transforms.Compose([\n","    transforms.RandomPerspective(fill=0, p=1,distortion_scale=0.5),\n","    transforms.Resize(224)\n","])\n","\n","source = \"a Photo\"\n","prompt = args.text"]},{"cell_type":"code","source":["# change optimize to False to run baseline\n","# change optimize to True to run optimized version\n","# run 1024 x 1024, please change peo_num to 0.1"],"metadata":{"id":"_k97u50tRHb9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dy-BfkW1CASU"},"outputs":[],"source":["optimize = True # whether use optimized loss\n","people_scale = 0.3 # penalty of potrait\n","back_scale = 1.0 # penalty of back\n","window_width = 0.2 # portion of patch size to determine area\n","back_thres = 0.7 # thres rejection of potrait\n","people_thres = 0.7 # thres rejection of back\n","peo_num = 0.2 # portion of patch in potrait"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7I7QABl7GI8r"},"outputs":[],"source":["mask = torch.tensor(np.repeat((seg.reshape(1, 1, 512, 512) == 15), 3, axis=1)).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7SBHqCyjnQMo"},"outputs":[],"source":["with torch.no_grad():\n","\n","    template_text = compose_text_with_templates(prompt, imagenet_templates)\n","    tokens = clip.tokenize(template_text).to(device)\n","    text_features = clip_model.encode_text(tokens).detach()\n","    text_features = text_features.mean(axis=0, keepdim=True)\n","    text_features /= text_features.norm(dim=-1, keepdim=True)\n","    \n","    template_source = compose_text_with_templates(source, imagenet_templates)\n","    tokens_source = clip.tokenize(template_source).to(device)\n","    text_source = clip_model.encode_text(tokens_source).detach()\n","    text_source = text_source.mean(axis=0, keepdim=True)\n","    text_source /= text_source.norm(dim=-1, keepdim=True)\n","\n","    source_features1 = clip_model.encode_image(clip_normalize(content_image.masked_fill(mask, 0), device))\n","    source_features1 /= (source_features1.clone().norm(dim=-1, keepdim=True))\n","    source_features = clip_model.encode_image(clip_normalize(content_image, device))\n","    source_features /= (source_features.clone().norm(dim=-1, keepdim=True))\n","\n","num_crops = args.num_crops\n","img_size = args.img_size\n","\n","for epoch in range(0, steps+1):\n","    \n","    target = style_net(content_image,use_sigmoid=True).to(device)\n","    target.requires_grad_(True)\n","    \n","    target_features = utils.get_features(img_normalize(target), VGG)\n","    content_loss = 0\n","\n","    content_loss += torch.mean((target_features['conv4_2'] - content_features['conv4_2']) ** 2)\n","    content_loss += torch.mean((target_features['conv5_2'] - content_features['conv5_2']) ** 2)\n","\n","    if optimize:\n","        back_proc, peo_proc, thres, scales =[], [], [], []\n","        while (len(back_proc) + len(peo_proc)) != args.num_crops:\n","            (i, j, h, w) = cropper.get_params(target, (crop_size, crop_size))\n","            target_crop = transforms.functional.crop(target, i, j, h, w)\n","            target_crop = augment(target_crop) \n","            if 15 in seg[i+h-int(h * window_width):i+h, j:j+w]: # potrait\n","                if len(peo_proc) < int(args.num_crops * peo_num):\n","                    peo_proc.append(target_crop)\n","                    scales.append(people_scale)\n","                    thres.append(people_thres)\n","            else: # background\n","                back_proc.append(target_crop)\n","                scales.append(back_scale)\n","                thres.append(back_thres)\n","        img_proc = back_proc + peo_proc\n","    else:\n","        img_proc = []\n","        for i in range(args.num_crops):\n","            (i, j, h, w) = cropper.get_params(target, (crop_size, crop_size))\n","            target_crop = transforms.functional.crop(target, i, j, h, w)\n","            target_crop = augment(target_crop)\n","            img_proc.append(target_crop)\n","\n","    img_proc = torch.cat(img_proc,dim=0)\n","    img_aug = img_proc\n","\n","    image_features = clip_model.encode_image(clip_normalize(img_aug,device))\n","    image_features /= (image_features.clone().norm(dim=-1, keepdim=True))\n","    \n","    img_direction = (image_features-source_features)\n","    img_direction /= img_direction.clone().norm(dim=-1, keepdim=True)\n","    \n","    text_direction = (text_features-text_source).repeat(image_features.size(0),1)\n","    text_direction /= text_direction.norm(dim=-1, keepdim=True)\n","    loss_temp = (1 - torch.cosine_similarity(img_direction, text_direction, dim=1))\n","    \n","    loss_patch = 0.0\n","    if optimize:\n","        for index, loss in enumerate(loss_temp):\n","            if loss >= thres[index]: \n","                loss_patch += loss * scales[index]\n","        loss_patch /= num_crops\n","    else:\n","        for index, loss in enumerate(loss_temp):\n","            if loss >= args.thresh:\n","                loss_patch += loss\n","        loss_patch /= num_crops\n","    \n","    if optimize:\n","        glob_features = clip_model.encode_image(clip_normalize(target.masked_fill(mask, 0),device))\n","        glob_features /= (glob_features.clone().norm(dim=-1, keepdim=True))\n","        glob_direction = (glob_features-source_features1)\n","        glob_direction /= glob_direction.clone().norm(dim=-1, keepdim=True)\n","    else:\n","        glob_features = clip_model.encode_image(clip_normalize(target,device))\n","        glob_features /= (glob_features.clone().norm(dim=-1, keepdim=True))\n","        glob_direction = (glob_features-source_features)\n","        glob_direction /= glob_direction.clone().norm(dim=-1, keepdim=True)\n","    \n","    loss_glob = (1 - torch.cosine_similarity(glob_direction, text_direction, dim=1)).mean()\n","    \n","    reg_tv = args.lambda_tv * get_image_prior_losses(target)\n","    total_loss = args.lambda_patch * loss_patch + content_weight * content_loss + reg_tv + args.lambda_dir * loss_glob\n","    total_loss_epoch.append(total_loss)\n","\n","    optimizer.zero_grad()\n","    total_loss.backward()\n","    optimizer.step()\n","    scheduler.step()\n","\n","    if epoch % 20 == 0:\n","        print(\"After %d iters:\" % epoch)\n","        print('Total loss: ', total_loss.item())\n","        print('Content loss: ', content_loss.item())\n","        print('patch loss: ', loss_patch.item())\n","        print('dir loss: ', loss_glob.item())\n","        print('TV loss: ', reg_tv.item())\n","    \n","    if epoch % 20 ==0:\n","        output_image = target.clone()\n","        output_image = torch.clamp(output_image,0,1)\n","        output_image = adjust_contrast(output_image,1.5)\n","        plt.figure(figsize=(15,15))\n","        plt.imshow(utils.im_convert2(output_image))\n","        plt.show()\n","    \n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sheF3TXyZuxn"},"outputs":[],"source":["torch.cuda.empty_cache()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Generative Artisan.ipynb","provenance":[{"file_id":"1dg8PXi-TVtzdpbaoI7ty72SSY7xdBgwo","timestamp":1645968498956}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}